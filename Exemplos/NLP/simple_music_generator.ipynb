{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Aquele 1% é probabilidade </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse tutorial vamos ver uma maneira bem simples de gerar textos automaticamente, pra isso utilizaremos probabilidade condicional e como fonte de dados músicas do Wesley Safadão :p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa é uma técnica bem simples, baseada na propriedade de Markov, que diz que em um processo aleatório de tempo discreto, a distribuição de probabilidade condicional para o próximo passo ($t+1$) depende apenas do estado atual, e não dos momentos anteriores. (Sabemos que essa prorpiedade não é 100% válida para textos, já que existe uma grande dependência entre os assuntos, mas ela pode nos retornar resultados interessantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além do uso de probabilidade condicional para gerar textos, também vamos brincar um pouco com a estrutura de dados do python, usando dicionários para acessar de forma eficiente a distribuiçao de probabilidade das lestras de Wesley. Também temos uma breve introdução da biblioteca lxml.html, o qual otilizamos para parsear as letras de música do letras.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as parser\n",
    "import requests\n",
    "import urllib\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos montar uma estrutura (json) para armazenarmos as os nomes e letras de cada música, que será nossa base de dados.\n",
    "O código pode parecer um pouco confuso, porém é simples. </br>\n",
    "\n",
    "De iníco vamos fazer um request na nossa url-base, onde estao listadas todas as músicas do autor que estamos procurando </br>\n",
    "Dentro da classe cnt-list, no html da página base, estão listados todos os links para as letras do autos, desta forma vamos gerar uma lista com todos esse links para podermos navegar por eles. </br>\n",
    "Por último, chamos a função get_song(), a qual enviamos o link de cada música e e obtemos a letra que está no div cnt-letra p402_premium. </br>\n",
    "\n",
    "Nosso dicionário songs tera a forma {nome_musica: lista de versos da musica}, como pode ser observado abaixo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_songs(autor):\n",
    "    def get_song(url):\n",
    "        r = requests.get(url) \n",
    "        song_html = parser.fromstring(r.text)\n",
    "        song = song_html.xpath(\"//div[@class='cnt-letra p402_premium']/article/p/text()\")\n",
    "        return song\n",
    "    \n",
    "    filename = autor+'.json'\n",
    "    if path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        base_url = urllib.parse.urljoin('https://www.letras.mus.br/', autor)\n",
    "        r = requests.get(start_url)\n",
    "        html = parser.fromstring(r.text)\n",
    "        links = html.xpath(\"//ul[@class='cnt-list']/li/a/@href\")\n",
    "        links = [urllib.parse.urljoin(base_url,l) for l in links]\n",
    "\n",
    "        songs = {song_url.split('/')[-2]: get_song(song_url) \n",
    "                 for song_url in links}\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(songs, file, ensure_ascii=False)\n",
    "        \n",
    "    return songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_dict = load_songs('wesley-safadao')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assim é o nosso amor', 'io io io, io io iooo', '100% amor']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_dict['100-amor'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10% de Red Bull', '10% de água de coco', '80% de uísque']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_dict['100-muito-louco'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos uma função para limpar levemente os dados, removendo apenas pontuações e textos entre parênteses (bis), (3X). Também adicionamos dois marcadores em cada verso, o > que marca início de uma sentença e <, que marca o fim.\n",
    "Desta forma, esses caracteres no auxiliaram a identificar quando uma frase deve ser completa, pois calcularemos a probabilidade de a frase terminar, dado que obtemos a palavra $w$, $P(<|w)$ e também forcecerá informação para sabermos se a frase está iniciando, ou seja, qual a probabilidade de uma palavra ocorrer, dado que é início da frase $P(>|w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    text = '> ' + text + ' <'\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como para nós o que importa são os versos, e não a letra toda, vamos agrupar todas as músicas em uma lista de versos. Vamos utilizar o comando chain, da itertools para desacoplarmos as sublistas em apenas uma lista. Após isso, dividimos os versos em tokens (palavras), pois precisaremos contar a ocorrência de cada uma no texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = list(chain(*songs_dict.values()))\n",
    "tokenized_corpora = [process_song(verse) for verse in corpora]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['>', 'assim', 'é', 'o', 'nosso', 'amor', '<'],\n",
       " ['>', 'io', 'io', 'io', 'io', 'io', 'iooo', '<'],\n",
       " ['>', '100', 'amor', '<']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpora[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para calcularmos a probabilidade de um item A, dado que ocorreu B, precisamos utilizar a regra de Bayes $P(A|B) = \\frac{P(A\\cap B)}{P(B)}$ </br>\n",
    "Para isso, vamos criar dois dicionários do tipo defauldict para armazenar as ocorrêncas $(A)$ e coocorrências $A\\cap B)$ dos tokens no texto. O defaultdict nos ajuda nessa tarefa, uma vez que solicitada uma chave não presente no nosso dicionário, ele retorna um valor default, 0 no nosso caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccurencies = defaultdict(lambda: 0)\n",
    "occurencies = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varremos todo nosso corpora com uma janela deslizante de tamanho dois, ou seja, para o verso ['>', 'assim', 'é', 'o', 'nosso', 'amor', '<'] teremos os tokens (>, assim), (assim, é), (é, o), .... que é a ocorrência do segundo item, dado que o primeiro apareceu.\n",
    "Vamos contar todas essas coocorrências e a ocorrência de cada token individalmente, uma vez que precisamos desses valores para calcular nossa probabilidade com a equação de Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for verse in tokenized_corpora:\n",
    "    for i in range(len(verse)-1):\n",
    "        cooccurencies[(verse[i], verse[i+1])] += 1\n",
    "        occurencies[verse[i]] += 1\n",
    "occurencies['<'] = occurencies['>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccurencies[('assim', 'é')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurencies['assim']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos salvar todas as nossas palavras do vocabulário em uma lista, pois precisaremos montar uma função de probabilidade para cada palavra do vocabulário e acessá-la depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(occurencies.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos montar uma outra estrutura para armazenar a probabilidade conjunta de cada palavra $w_i$ ocorrer, dada a palavra $w_{i-1}$. Será um dicionário de dicionários, onde a chave mais externa corresponde à palavra dada e a mais interna à probabilidade de ela ocorrer condicionada à anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_probs = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for condition in cooccurencies.keys():\n",
    "    condition_probs[condition[0]][condition[1]] = cooccurencies[condition]/occurencies[condition[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000947205743057979"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_probs['>']['assim']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e a probabilidade de 'é', dado que 'assim' ocorreu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0313588850174216"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_probs['assim']['é']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora está muito simples gerarmos nossos textos, uma vez que temos a função de distribuição de probabilidade de todos os pares de palavra, precisamos apenas sorteá-las de acordo com a a probabilidade de cada uma ocorrer, o que alterado a cada novo sorteio, já que a palavra $w_i$ fornece informação sobre a ocorrência da palavra $w_{i+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no forró que tá feliz daqui pra lá chega em poucas palavras <\n",
      "o seu travesseiro <\n",
      "seus lábios de ser assim me vingar vingar vingar vingar <\n",
      "que fazes <\n",
      "tão difícil pra ganhar teu coração <\n",
      "nem muito por ti esquecer <\n",
      "e me quer cantar alegremente <\n",
      "minha mente é tudo bem eu sou apaixonada <\n",
      "vou me ver quem não eu sou o que é lindo dono do seu corpo fica ai <\n",
      "o desmantelo <\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fact = '>'\n",
    "    verse = []\n",
    "    while fact != '<':\n",
    "        probs = [condition_probs[fact][token] for token in vocabulary]\n",
    "        fact = np.random.choice(vocabulary, 1, p=probs)[0]\n",
    "        verse.append(fact)\n",
    "    print(' '.join(verse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probs é nossa lista de probabilidade de ocorrência para cada palavra do vocabulário, note que ela é refeita a cada palavra (fact) nova que sorteamos, pois isso alterá a probabilidade das próximas palavras $P(w_{i+1}|w_i)$. Essas probabilidade condicionais estão todas amrazenadas no nosso dicionário condition_probs.\n",
    "\n",
    "A função np.random.choice sorteia um item da lista vocabulary, de acordo com a probabilidade de cada uma, definida no vetor probs.\n",
    "Poderíamos sempre selecionar a palavra mais provável, porém perderíamos a dinâmica e estocacidade do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de ser um exemplo bem simples e não nos retornar nenhum resultado digno de um gremmy, é possível observar que as frases fazem mais sentido do que palavras aleatórias.\n",
    "\n",
    "Esta técnica pode ser melhorada facilmente utilizando-se bigramas, ou gerando dois versos, condicionando a última palavra do segundo verso à última do primeiro (para mantermos a rima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
